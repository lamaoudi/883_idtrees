{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from idtrees.utils import read_tifs #, load_data # Import data utils\n",
    "from idtrees.utils.get_data import *\n",
    "import matplotlib.pyplot as plt\n",
    "from configs import *\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting ITCs as differently sized HSI images with a Tree Species Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees, labeled with species and bounding box:  1165\n",
      "[WARNING] Loaded box has zero shape and is sketchily inflated. TODO: skip this box with ID 749\n",
      "[WARNING] Loaded box has zero shape and is sketchily inflated. TODO: skip this box with ID 1046\n",
      "Classes to be used: [23. 26. 22.  2. 30. 18.]\n",
      "Counts for these classes [ 53.  97. 103. 139. 169. 367.]\n",
      "Number of trees, labeled with species and bounding box:  1165\n",
      "[WARNING] Loaded box has zero shape and is sketchily inflated. TODO: skip this box with ID 749\n",
      "[WARNING] Loaded box has zero shape and is sketchily inflated. TODO: skip this box with ID 1046\n"
     ]
    }
   ],
   "source": [
    "im_all_new, new_class_ids, class_id_val, n_px_val, sci_names, special_val_px = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = [] # List of spectrum per pixel # TODO write as ndarray\n",
    "class_id_new = [] # List of target per pixel \n",
    "\n",
    "for index in range(len(im_all_new)):\n",
    "    # Append the spectra and class id of all pixels in bbox to a list\n",
    "    n_px = np.prod(im_all_new[index].shape[1:])\n",
    "    spectra.append(im_all_new[index].reshape(-1, n_px))\n",
    "    class_id_new.append(new_class_ids[index] * np.ones(n_px))\n",
    "\n",
    "# Convert list into ndarray\n",
    "spectra = np.concatenate(spectra, axis=1)#.numpy())\n",
    "class_id_new = np.concatenate(class_id_new, axis=0)\n",
    "\n",
    "# Add class ids as zero'th row \n",
    "pixel_data = np.vstack((class_id_new[np.newaxis,:], spectra))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'class_ids': class_id_val, 'num_pix': n_px_val, 'sci_names': sci_names})\n",
    "freq.loc[6, :] = ['34.', str(np.sum(special_val_px)), \"ALL OTHER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "n_train = int(.8 * pixel_data.shape[1])\n",
    "xy = np.rollaxis(pixel_data, 1) # Format X into (n_samples, n_features)\n",
    "np.random.shuffle(xy) # Shuffle randomly along axis of n_samples \n",
    "X = xy[:, 1:] \n",
    "Y = xy[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after [PCA] (35488, 360)\n"
     ]
    }
   ],
   "source": [
    "# Do PCA\n",
    "do_pca = True\n",
    "if do_pca:\n",
    "    pca = PCA(n_components='mle', whiten=False, svd_solver='full')\n",
    "    X = pca.fit_transform(X)\n",
    "print('Shape after [PCA]', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:n_train, :]\n",
    "y_train = Y[:n_train]\n",
    "X_test = X[n_train:, :]\n",
    "y_test = Y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "poly = svm.SVC(kernel='poly', degree=3, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "sig = svm.SVC(kernel='sigmoid', C=1, decision_function_shape='ovo').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Linear Kernel: 0.6108763031839954\n",
      "Accuracy Polynomial Kernel: 0.503803888419273\n",
      "Accuracy Radial Basis Kernel: 0.6107354184277262\n",
      "Accuracy Sigmoid Kernel: 0.2948717948717949\n",
      "[[ 362    2   10    0    0  306  231]\n",
      " [   0 1732    0    0   45    0   21]\n",
      " [  79    0  278    0    0  319  175]\n",
      " [  42    0    3    0    0  270   96]\n",
      " [   0  167    0    0  211    0    3]\n",
      " [  51    0   29    0    0  970  186]\n",
      " [ 160   46   92    0    8  421  783]]\n",
      "[[  77    0    4    0    0  608  222]\n",
      " [   1 1525    0    0   25  218   29]\n",
      " [  17    0   71    0    0  472  291]\n",
      " [   4    2    1    0    0  333   71]\n",
      " [   0  199    0    0  138   40    4]\n",
      " [  15    8    2    0    0 1033  178]\n",
      " [  28   52   18    0    1  679  732]]\n",
      "[[ 339    2   18    0    0  279  273]\n",
      " [   1 1751    0    0   30    0   16]\n",
      " [  81    0  260    0    0  278  232]\n",
      " [  66    2   23    6    0  217   97]\n",
      " [   0  215    0    0  163    0    3]\n",
      " [  80    3   49    1    0  918  185]\n",
      " [ 162   45   80    0    1  324  898]]\n",
      "[[ 186   30  305    0    0  275  115]\n",
      " [ 302 1079  197    0   40  129   51]\n",
      " [ 173   23  255    0    0  296  104]\n",
      " [  85   21  143    0    0  109   53]\n",
      " [  60  242   18    0   17   37    7]\n",
      " [ 256   91  372    0    0  381  136]\n",
      " [ 353   92  454    0    2  434  175]]\n"
     ]
    }
   ],
   "source": [
    "linear_pred = linear.predict(X_test)\n",
    "poly_pred = poly.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "sig_pred = sig.predict(X_test)\n",
    "\n",
    "# retrieve the accuracy and print it for all 4 kernel functions\n",
    "accuracy_lin = linear.score(X_test, y_test)\n",
    "accuracy_poly = poly.score(X_test, y_test)\n",
    "accuracy_rbf = rbf.score(X_test, y_test)\n",
    "accuracy_sig = sig.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy Linear Kernel:\", accuracy_lin)\n",
    "print(\"Accuracy Polynomial Kernel:\", accuracy_poly)\n",
    "print('Accuracy Radial Basis Kernel:', accuracy_rbf)\n",
    "print('Accuracy Sigmoid Kernel:', accuracy_sig)\n",
    "      \n",
    "# creating a confusion matrix\n",
    "cm_lin = confusion_matrix(y_test, linear_pred)\n",
    "cm_poly = confusion_matrix(y_test, poly_pred)\n",
    "cm_rbf = confusion_matrix(y_test, rbf_pred)\n",
    "cm_sig = confusion_matrix(y_test, sig_pred)\n",
    "      \n",
    "print(cm_lin)\n",
    "print(cm_poly)\n",
    "print(cm_rbf)\n",
    "print(cm_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rebalanced, removing Other, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets normalize and center the data\n",
    "X_std = X - X.mean(axis=1).reshape(-1,1)\n",
    "X_std = X_std/X_std.std(axis=1).reshape(-1,1)\n",
    "\n",
    "## Lets leave out 34\n",
    "X_std = X_std[Y != 34, :]\n",
    "Y_std = Y[Y!=34]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_std, Y_std, test_size=0.2, random_state=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22468, 360), (22468,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_data(X_ub, Y_ub, up_balance_scale=3.):\n",
    "    # Get classes and counts\n",
    "    vals, counts = np.unique(Y_ub, return_counts=True)\n",
    "    # See how bad the inbalance is and choose n_choose according to up_balance_scale\n",
    "    if counts.max()/counts.min() >up_balance_scale:\n",
    "        n_choose = int(counts.min()*up_balance_scale)\n",
    "    else:\n",
    "        n_choose = int(counts.max())\n",
    "\n",
    "    X_new = np.zeros((n_choose*vals.shape[0], X_ub.shape[1]))\n",
    "    Y_new = np.zeros(n_choose*vals.shape[0])\n",
    "    for i, val in enumerate(vals):\n",
    "        bool_arr = (Y_ub == val)\n",
    "        if n_choose-bool_arr.sum()<0:\n",
    "            random_idxs = np.random.randint(counts[i], size=(n_choose))\n",
    "            X_new[i*n_choose:(i+1)*n_choose,:] = X_ub[bool_arr, :][random_idxs,:]\n",
    "        else:\n",
    "            random_idxs = np.random.randint(counts[i], size=(n_choose-bool_arr.sum()))\n",
    "            X_new[i*n_choose:(i+1)*n_choose,:] = np.concatenate((X_ub[bool_arr, :],X_ub[bool_arr, :][random_idxs,:]),\n",
    "                                                                axis=0)\n",
    "        Y_new[i*n_choose:(i+1)*n_choose] = val\n",
    "    return X_new, Y_new\n",
    "X_train, y_train = rebalance_data(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Re-run SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Linear Kernel: 0.7062488873063913\n",
      "Accuracy Radial Basis Kernel: 0.6150970268826775\n",
      "[[ 484    1   81  138    0  185]\n",
      " [   0 1544    0    0  195    0]\n",
      " [ 134    0  533  135    0  118]\n",
      " [  42    0   23  284    0   43]\n",
      " [   0   35    0    0  380    0]\n",
      " [ 161    3  118  238    0  742]]\n",
      "[[ 425    1   75   78    0  310]\n",
      " [   1 1240    0    1  114  383]\n",
      " [ 150    0  416   51    0  303]\n",
      " [  92    0   30  162    0  108]\n",
      " [   0   72    0    0  306   37]\n",
      " [ 165    1  156   34    0  906]]\n"
     ]
    }
   ],
   "source": [
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "rbf = svm.SVC(kernel='rbf', gamma=1, C=1, decision_function_shape='ovo').fit(X_train, y_train)\n",
    "\n",
    "linear_pred = linear.predict(X_test)\n",
    "rbf_pred = rbf.predict(X_test)\n",
    "\n",
    "# retrieve the accuracy and print it for all 4 kernel functions\n",
    "accuracy_lin = linear.score(X_test, y_test)\n",
    "accuracy_rbf = rbf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy Linear Kernel:\", accuracy_lin)\n",
    "print('Accuracy Radial Basis Kernel:', accuracy_rbf)\n",
    "      \n",
    "# creating a confusion matrix\n",
    "cm_lin = confusion_matrix(y_test, linear_pred)\n",
    "cm_rbf = confusion_matrix(y_test, rbf_pred)\n",
    "      \n",
    "print(cm_lin)\n",
    "print(cm_rbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running new models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg LogisticRegression(max_iter=10000.0) ==========\n",
      "LogReg\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Acer rubrum L.       0.59      0.55      0.57       889\n",
      "Pinus palustris Mill.       0.98      0.88      0.93      1739\n",
      "      Quercus alba L.       0.66      0.60      0.63       920\n",
      "     Quercus coccinea       0.34      0.65      0.44       392\n",
      "Quercus laevis Walter       0.65      0.93      0.77       415\n",
      "     Quercus rubra L.       0.68      0.56      0.62      1262\n",
      "\n",
      "             accuracy                           0.70      5617\n",
      "            macro avg       0.65      0.70      0.66      5617\n",
      "         weighted avg       0.73      0.70      0.71      5617\n",
      "\n",
      "[[ 491    2   95  137    0  164]\n",
      " [   0 1535    0    1  203    0]\n",
      " [ 141    0  554  117    0  108]\n",
      " [  47    0   32  253    0   60]\n",
      " [   0   29    0    0  386    0]\n",
      " [ 149    2  157  241    2  711]]\n",
      "RF RandomForestClassifier(n_jobs=4) ==========\n",
      "RF\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Acer rubrum L.       0.51      0.51      0.51       889\n",
      "Pinus palustris Mill.       0.94      0.95      0.94      1739\n",
      "      Quercus alba L.       0.61      0.53      0.57       920\n",
      "     Quercus coccinea       0.51      0.40      0.45       392\n",
      "Quercus laevis Walter       0.81      0.80      0.81       415\n",
      "     Quercus rubra L.       0.59      0.68      0.63      1262\n",
      "\n",
      "             accuracy                           0.70      5617\n",
      "            macro avg       0.66      0.65      0.65      5617\n",
      "         weighted avg       0.70      0.70      0.70      5617\n",
      "\n",
      "[[ 457    7  100   72    0  253]\n",
      " [   3 1652    0    0   80    4]\n",
      " [ 158    5  488   44    0  225]\n",
      " [  94    7   27  157    0  107]\n",
      " [   3   78    0    0  334    0]\n",
      " [ 179   16  179   32    0  856]]\n",
      "XGB XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=None, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None) ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:31:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:59:20] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:12:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ozaltun/.local/lib/python3.8/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:27:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGB\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Acer rubrum L.       0.53      0.54      0.53       889\n",
      "Pinus palustris Mill.       0.97      0.94      0.96      1739\n",
      "      Quercus alba L.       0.63      0.58      0.60       920\n",
      "     Quercus coccinea       0.53      0.53      0.53       392\n",
      "Quercus laevis Walter       0.79      0.88      0.83       415\n",
      "     Quercus rubra L.       0.66      0.69      0.67      1262\n",
      "\n",
      "             accuracy                           0.73      5617\n",
      "            macro avg       0.69      0.69      0.69      5617\n",
      "         weighted avg       0.73      0.73      0.73      5617\n",
      "\n",
      "[[ 478    3  106   87    0  215]\n",
      " [   2 1639    0    0   98    0]\n",
      " [ 155    0  532   51    0  182]\n",
      " [  86    0   37  209    0   60]\n",
      " [   0   49    0    0  366    0]\n",
      " [ 177    0  165   46    0  874]]\n",
      "KNN KNeighborsClassifier(n_jobs=4) ==========\n",
      "KNN\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Acer rubrum L.       0.47      0.53      0.50       889\n",
      "Pinus palustris Mill.       0.96      0.79      0.87      1739\n",
      "      Quercus alba L.       0.59      0.55      0.57       920\n",
      "     Quercus coccinea       0.36      0.61      0.45       392\n",
      "Quercus laevis Walter       0.50      0.87      0.64       415\n",
      "     Quercus rubra L.       0.68      0.51      0.58      1262\n",
      "\n",
      "             accuracy                           0.64      5617\n",
      "            macro avg       0.59      0.64      0.60      5617\n",
      "         weighted avg       0.68      0.64      0.65      5617\n",
      "\n",
      "[[ 469    0  114  157    1  148]\n",
      " [   6 1372    0    3  356    2]\n",
      " [ 190    0  507  114    0  109]\n",
      " [  81    2   33  239    0   37]\n",
      " [   1   48    0    0  363    3]\n",
      " [ 256    4  200  154    5  643]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "'''\n",
    "Lightweight script to test many models and find winners\n",
    ":param X_train: training split\n",
    ":param y_train: training target vector\n",
    ":param X_test: test split\n",
    ":param y_test: test target vector\n",
    ":return: DataFrame of predictions\n",
    "'''\n",
    "\n",
    "dfs = []\n",
    "models = [('LogReg', LogisticRegression(max_iter=1e4)), \n",
    "          ('RF', RandomForestClassifier(n_jobs=4)),\n",
    "          ('XGB', XGBClassifier()),\n",
    "          ('KNN', KNeighborsClassifier(n_jobs=4)),\n",
    "          ('SVM', SVC(kernel='linear', C=1, decision_function_shape='ovo'))\n",
    "        ]\n",
    "results = []\n",
    "names = []\n",
    "scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "target_names = sci_names\n",
    "for name, model in models:\n",
    "    print(name, model, \"=\"*10)\n",
    "    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "    cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(name)\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    this_df = pd.DataFrame(cv_results)\n",
    "    this_df['model'] = name\n",
    "    dfs.append(this_df)\n",
    "final = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM SVC(C=1, decision_function_shape='ovo', kernel='linear') ==========\n",
      "SVM\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       Acer rubrum L.       0.59      0.54      0.57       889\n",
      "Pinus palustris Mill.       0.98      0.89      0.93      1739\n",
      "      Quercus alba L.       0.71      0.58      0.64       920\n",
      "     Quercus coccinea       0.36      0.72      0.48       392\n",
      "Quercus laevis Walter       0.66      0.92      0.77       415\n",
      "     Quercus rubra L.       0.68      0.59      0.63      1262\n",
      "\n",
      "             accuracy                           0.71      5617\n",
      "            macro avg       0.66      0.71      0.67      5617\n",
      "         weighted avg       0.74      0.71      0.71      5617\n",
      "\n",
      "[[ 484    1   81  138    0  185]\n",
      " [   0 1544    0    0  195    0]\n",
      " [ 134    0  533  135    0  118]\n",
      " [  42    0   23  284    0   43]\n",
      " [   0   35    0    0  380    0]\n",
      " [ 161    3  118  238    0  742]]\n"
     ]
    }
   ],
   "source": [
    "name = 'SVM'\n",
    "model = SVC(kernel='linear', C=1, decision_function_shape='ovo')\n",
    "\n",
    "print(name, model, \"=\"*10)\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "clf = model.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(name)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "results.append(cv_results)\n",
    "names.append(name)\n",
    "this_df = pd.DataFrame(cv_results)\n",
    "this_df['model'] = name\n",
    "dfs.append(this_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision_weighted</th>\n",
       "      <th>test_recall_weighted</th>\n",
       "      <th>test_f1_weighted</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.729349</td>\n",
       "      <td>0.029511</td>\n",
       "      <td>0.715530</td>\n",
       "      <td>0.716101</td>\n",
       "      <td>0.715530</td>\n",
       "      <td>0.715391</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.655378</td>\n",
       "      <td>0.028529</td>\n",
       "      <td>0.710504</td>\n",
       "      <td>0.710959</td>\n",
       "      <td>0.710504</td>\n",
       "      <td>0.710007</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.289276</td>\n",
       "      <td>0.032071</td>\n",
       "      <td>0.704976</td>\n",
       "      <td>0.706142</td>\n",
       "      <td>0.704976</td>\n",
       "      <td>0.703751</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104.327832</td>\n",
       "      <td>0.030897</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.703738</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.703217</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.021823</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>0.707942</td>\n",
       "      <td>0.708518</td>\n",
       "      <td>0.707942</td>\n",
       "      <td>0.707182</td>\n",
       "      <td>LogReg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.726020</td>\n",
       "      <td>0.153408</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.815698</td>\n",
       "      <td>0.816552</td>\n",
       "      <td>0.815975</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.812462</td>\n",
       "      <td>0.134350</td>\n",
       "      <td>0.818060</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.818060</td>\n",
       "      <td>0.816777</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.007772</td>\n",
       "      <td>0.126580</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.811490</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.811820</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.695639</td>\n",
       "      <td>0.121526</td>\n",
       "      <td>0.821746</td>\n",
       "      <td>0.821551</td>\n",
       "      <td>0.821746</td>\n",
       "      <td>0.821416</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.850593</td>\n",
       "      <td>0.126625</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>0.816666</td>\n",
       "      <td>0.817527</td>\n",
       "      <td>0.816726</td>\n",
       "      <td>RF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>476.863387</td>\n",
       "      <td>0.065422</td>\n",
       "      <td>0.823756</td>\n",
       "      <td>0.822691</td>\n",
       "      <td>0.823756</td>\n",
       "      <td>0.823155</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>522.498283</td>\n",
       "      <td>0.075017</td>\n",
       "      <td>0.826939</td>\n",
       "      <td>0.825351</td>\n",
       "      <td>0.826939</td>\n",
       "      <td>0.825723</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>700.676077</td>\n",
       "      <td>0.056391</td>\n",
       "      <td>0.825431</td>\n",
       "      <td>0.824241</td>\n",
       "      <td>0.825431</td>\n",
       "      <td>0.824303</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>766.723400</td>\n",
       "      <td>0.065143</td>\n",
       "      <td>0.826269</td>\n",
       "      <td>0.825230</td>\n",
       "      <td>0.826269</td>\n",
       "      <td>0.825556</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>899.550511</td>\n",
       "      <td>0.087312</td>\n",
       "      <td>0.826072</td>\n",
       "      <td>0.824590</td>\n",
       "      <td>0.826072</td>\n",
       "      <td>0.824993</td>\n",
       "      <td>XGB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.065176</td>\n",
       "      <td>6.202654</td>\n",
       "      <td>0.715195</td>\n",
       "      <td>0.715554</td>\n",
       "      <td>0.715195</td>\n",
       "      <td>0.711959</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.044187</td>\n",
       "      <td>7.297048</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.718480</td>\n",
       "      <td>0.715363</td>\n",
       "      <td>0.712051</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.067715</td>\n",
       "      <td>6.963593</td>\n",
       "      <td>0.704808</td>\n",
       "      <td>0.706554</td>\n",
       "      <td>0.704808</td>\n",
       "      <td>0.701747</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.065985</td>\n",
       "      <td>6.759948</td>\n",
       "      <td>0.712850</td>\n",
       "      <td>0.713608</td>\n",
       "      <td>0.712850</td>\n",
       "      <td>0.710350</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.042339</td>\n",
       "      <td>6.912131</td>\n",
       "      <td>0.706937</td>\n",
       "      <td>0.710406</td>\n",
       "      <td>0.706937</td>\n",
       "      <td>0.703382</td>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>98.607210</td>\n",
       "      <td>23.038812</td>\n",
       "      <td>0.721896</td>\n",
       "      <td>0.723297</td>\n",
       "      <td>0.721896</td>\n",
       "      <td>0.721549</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>95.506714</td>\n",
       "      <td>22.389933</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.721661</td>\n",
       "      <td>0.719048</td>\n",
       "      <td>0.718422</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>91.923029</td>\n",
       "      <td>22.182190</td>\n",
       "      <td>0.714525</td>\n",
       "      <td>0.718371</td>\n",
       "      <td>0.714525</td>\n",
       "      <td>0.713644</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>92.829268</td>\n",
       "      <td>22.191671</td>\n",
       "      <td>0.712515</td>\n",
       "      <td>0.715752</td>\n",
       "      <td>0.712515</td>\n",
       "      <td>0.712319</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>94.187484</td>\n",
       "      <td>22.152006</td>\n",
       "      <td>0.713472</td>\n",
       "      <td>0.716370</td>\n",
       "      <td>0.713472</td>\n",
       "      <td>0.712720</td>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fit_time  score_time  test_accuracy  test_precision_weighted  \\\n",
       "0    86.729349    0.029511       0.715530                 0.716101   \n",
       "1    90.655378    0.028529       0.710504                 0.710959   \n",
       "2    90.289276    0.032071       0.704976                 0.706142   \n",
       "3   104.327832    0.030897       0.703300                 0.703738   \n",
       "4    91.021823    0.050027       0.707942                 0.708518   \n",
       "5    12.726020    0.153408       0.816552                 0.815698   \n",
       "6    11.812462    0.134350       0.818060                 0.815914   \n",
       "7    12.007772    0.126580       0.813034                 0.811490   \n",
       "8    11.695639    0.121526       0.821746                 0.821551   \n",
       "9    11.850593    0.126625       0.817527                 0.816666   \n",
       "10  476.863387    0.065422       0.823756                 0.822691   \n",
       "11  522.498283    0.075017       0.826939                 0.825351   \n",
       "12  700.676077    0.056391       0.825431                 0.824241   \n",
       "13  766.723400    0.065143       0.826269                 0.825230   \n",
       "14  899.550511    0.087312       0.826072                 0.824590   \n",
       "15    0.065176    6.202654       0.715195                 0.715554   \n",
       "16    0.044187    7.297048       0.715363                 0.718480   \n",
       "17    0.067715    6.963593       0.704808                 0.706554   \n",
       "18    0.065985    6.759948       0.712850                 0.713608   \n",
       "19    0.042339    6.912131       0.706937                 0.710406   \n",
       "20   98.607210   23.038812       0.721896                 0.723297   \n",
       "21   95.506714   22.389933       0.719048                 0.721661   \n",
       "22   91.923029   22.182190       0.714525                 0.718371   \n",
       "23   92.829268   22.191671       0.712515                 0.715752   \n",
       "24   94.187484   22.152006       0.713472                 0.716370   \n",
       "\n",
       "    test_recall_weighted  test_f1_weighted   model  \n",
       "0               0.715530          0.715391  LogReg  \n",
       "1               0.710504          0.710007  LogReg  \n",
       "2               0.704976          0.703751  LogReg  \n",
       "3               0.703300          0.703217  LogReg  \n",
       "4               0.707942          0.707182  LogReg  \n",
       "5               0.816552          0.815975      RF  \n",
       "6               0.818060          0.816777      RF  \n",
       "7               0.813034          0.811820      RF  \n",
       "8               0.821746          0.821416      RF  \n",
       "9               0.817527          0.816726      RF  \n",
       "10              0.823756          0.823155     XGB  \n",
       "11              0.826939          0.825723     XGB  \n",
       "12              0.825431          0.824303     XGB  \n",
       "13              0.826269          0.825556     XGB  \n",
       "14              0.826072          0.824993     XGB  \n",
       "15              0.715195          0.711959     KNN  \n",
       "16              0.715363          0.712051     KNN  \n",
       "17              0.704808          0.701747     KNN  \n",
       "18              0.712850          0.710350     KNN  \n",
       "19              0.706937          0.703382     KNN  \n",
       "20              0.721896          0.721549     SVM  \n",
       "21              0.719048          0.718422     SVM  \n",
       "22              0.714525          0.713644     SVM  \n",
       "23              0.712515          0.712319     SVM  \n",
       "24              0.713472          0.712720     SVM  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat(dfs, ignore_index=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"../output/model_runs_BO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([98.60721016, 95.50671411, 91.92302942, 92.8292675 , 94.18748379]),\n",
       " 'score_time': array([23.03881216, 22.38993311, 22.18219018, 22.19167137, 22.15200639]),\n",
       " 'test_accuracy': array([0.72189647, 0.71904842, 0.71452505, 0.71251466, 0.71347185]),\n",
       " 'test_precision_weighted': array([0.72329681, 0.72166124, 0.71837132, 0.71575236, 0.71637033]),\n",
       " 'test_recall_weighted': array([0.72189647, 0.71904842, 0.71452505, 0.71251466, 0.71347185]),\n",
       " 'test_f1_weighted': array([0.72154857, 0.71842221, 0.71364389, 0.71231903, 0.71271978])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
